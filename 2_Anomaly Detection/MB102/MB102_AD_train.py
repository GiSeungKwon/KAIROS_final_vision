import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import glob
import time
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt

# =================================================================
# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ê²½ë¡œ ì„¤ì •
# =================================================================
BATCH_SIZE = 32
LEARNING_RATE = 1e-3
NUM_EPOCHS = 20
IMAGE_SIZE = 128
DATA_DIR = "../../../data/ObjectClassification/Aug_MB102"
# ìµœì  ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ì¡´ ìœ ì§€)
MODEL_SAVE_PATH_BEST = 'MB102_anomaly_detector_best_loss.pth' 
# ì£¼ê¸°ì  ë°±ì—… ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ìƒˆë¡œ ì¶”ê°€, í¬ë§· ë¬¸ìì—´ë¡œ ì‚¬ìš©)
MODEL_SAVE_PATH_PERIODIC = 'MB102_anomaly_detector_epoch_{:03d}.pth' 
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# â­â­ ì£¼ê¸°ì  ì €ì¥ ê°„ê²© ì„¤ì • â­â­
SAVE_INTERVAL_EPOCH = 5

print(f"ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

# =================================================================
# 2. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
# =================================================================
class CustomImageDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.image_paths = glob.glob(os.path.join(data_dir, '*.jpg')) 
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)
        
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
            
        return image

# =================================================================
# 3. ë°ì´í„° ë³€í™˜ ë° DataLoader ì„¤ì • (ë³€ê²½ ì—†ìŒ)
# =================================================================
data_transforms = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 
])

dataset = CustomImageDataset(DATA_DIR, transform=data_transforms)

if len(dataset) == 0:
    print(f"\n[ì˜¤ë¥˜ ë°œìƒ] ë°ì´í„°ì…‹ì— ì´ë¯¸ì§€ê°€ 0ê°œì…ë‹ˆë‹¤. ê²½ë¡œ({DATA_DIR})ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit() 

dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)

print(f"ë¡œë“œëœ MB102 ì •ìƒ ì´ë¯¸ì§€ ìˆ˜: {len(dataset)}")

# =================================================================
# 4. ëª¨ë¸ ì •ì˜ (Convolutional Autoencoder - ë³€ê²½ ì—†ìŒ)
# =================================================================
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=2, padding=1),   nn.ReLU(True), # 128 -> 64
            nn.Conv2d(16, 32, 3, stride=2, padding=1),  nn.ReLU(True), # 64 -> 32
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  nn.ReLU(True), # 32 -> 16
            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(True)  # 16 -> 8 (Latent Space: 128 x 8 x 8)
        )
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(True), # 8 -> 16
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(True),  # 16 -> 32
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), nn.ReLU(True),  # 32 -> 64
            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),                  # 64 -> 128 (Output: 3 x 128 x 128)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”, ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜ (ë³€ê²½ ì—†ìŒ)
model = Autoencoder().to(DEVICE)
criterion = nn.MSELoss() 
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# =================================================================
# 5. ëª¨ë¸ í•™ìŠµ ë£¨í”„ (ìˆ˜ì •ë¨: 5 ì—í¬í¬ ì£¼ê¸° ì €ì¥ ë¡œì§ ì¶”ê°€)
# =================================================================
def train_model(model, dataloader, criterion, optimizer, num_epochs, best_save_path, periodic_save_path_format, save_interval):
    print("--- ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---")
    start_time = time.time()
    
    epoch_losses = []
    best_loss = np.inf 
    
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        
        train_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')
        
        for inputs in train_bar:
            inputs = inputs.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, inputs)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * inputs.size(0)
            train_bar.set_postfix({'Loss': f'{loss.item():.6f}'})

        epoch_loss = running_loss / len(dataset)
        epoch_losses.append(epoch_loss)
        
        print(f"\n[Epoch {epoch+1}/{num_epochs}] í‰ê·  Loss: {epoch_loss:.6f}")
        
        # 1. ìµœì  ëª¨ë¸ ì €ì¥ ë¡œì§ (Loss ê°œì„  ì‹œ)
        if epoch_loss < best_loss:
            best_loss = epoch_loss
            torch.save(model.state_dict(), best_save_path)
            print(f"-> â­ ìµœì  ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ({best_save_path}ì— ì €ì¥ë¨, Loss: {best_loss:.6f}ë¡œ ê°ì†Œ)")

        # 2. â­â­ 5 ì—í¬í¬ ì£¼ê¸°ì  ë°±ì—… ì €ì¥ ë¡œì§ ì¶”ê°€ â­â­
        if (epoch + 1) % save_interval == 0:
            periodic_path = periodic_save_path_format.format(epoch + 1)
            torch.save(model.state_dict(), periodic_path)
            print(f"-> ğŸ’¾ ì£¼ê¸°ì  ë°±ì—… ì™„ë£Œ! ({epoch+1} ì—í¬í¬, {periodic_path}ì— ì €ì¥ë¨)")

    end_time = time.time()
    print(f"\n--- ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ ---")
    print(f"ìµœì¢… ì €ì¥ëœ ìµœì  ëª¨ë¸: {best_save_path} (ìµœì†Œ Loss: {best_loss:.6f})")

    # Loss ê·¸ë˜í”„ ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
    plot_loss_curve(epoch_losses, num_epochs, best_loss)
    
# =================================================================
# 6. Loss ê·¸ë˜í”„ ì‹œê°í™” í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
# =================================================================
def plot_loss_curve(losses, num_epochs, best_loss):
    """
    í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    epochs = range(1, num_epochs + 1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, losses, 'b-', label='Training Loss (MSE)')
    
    # ìµœì  ì†ì‹¤ ë¼ì¸ ì¶”ê°€
    min_loss_index = np.argmin(losses)
    min_epoch = min_loss_index + 1
    
    # ìµœì  ëª¨ë¸ ì €ì¥ ë¼ì¸ (min_epoch ìœ„ì¹˜ì— í‘œì‹œ)
    plt.axvline(x=min_epoch, color='r', linestyle='--', label=f'Best Model Saved (Epoch {min_epoch})')
    # í…ìŠ¤íŠ¸ê°€ ê·¸ë˜í”„ ìœ„ì— ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì¡°ì •
    text_y_position = max(losses) - (max(losses) - min(losses)) * 0.1 
    plt.text(min_epoch, text_y_position, f'Min Loss: {best_loss:.6f}', color='r', rotation=0, ha='center', va='bottom', fontsize=9)
    
    plt.title('Autoencoder Training Loss Convergence')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Squared Error (MSE) Loss')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()

# =================================================================
# 7. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =================================================================
if __name__ == '__main__':
    # Matplotlibì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ë³€ê²½ ì—†ìŒ)
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("Matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install matplotlib ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        exit()
        
    train_model(
        model, 
        dataloader, 
        criterion, 
        optimizer, 
        NUM_EPOCHS,
        MODEL_SAVE_PATH_BEST,         # ìµœì  ì†ì‹¤ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        MODEL_SAVE_PATH_PERIODIC,     # ì£¼ê¸°ì  ë°±ì—… ëª¨ë¸ íŒŒì¼ëª… í¬ë§·
        SAVE_INTERVAL_EPOCH           # ì£¼ê¸°ì  ì €ì¥ ê°„ê²© (5)
    )